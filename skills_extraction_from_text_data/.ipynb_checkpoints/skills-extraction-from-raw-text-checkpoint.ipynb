{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# Transformers and related libraries\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the job ads dataset and ESCO skills ontology\n",
    "I removed duplicates from the job ads, the dataset is still very big, I won't be using all the entries.\n",
    "<br>For the ESCO skills ontology, I will only be using the preferred labels and consider all entries as skills, even though ESCO divides them into subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBS_FP = 'marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv'\n",
    "ESCO_SKILLS_FP = 'skills_en.csv'\n",
    "\n",
    "\n",
    "df = pd.read_csv(JOBS_FP)\n",
    "df.drop_duplicates(subset=['Job Description'], keep='first', inplace=True)\n",
    "esco_df = pd.read_csv(ESCO_SKILLS_FP)\n",
    "esco_df['label_cleaned'] = esco_df['preferredLabel'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x).strip())\n",
    "esco_df['word_cnt'] = esco_df['label_cleaned'].apply(lambda x: len(str(x).split()))\n",
    "esco_df = pd.DataFrame(esco_df, columns=['label_cleaned', 'altLabels', 'word_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13896, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esco_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EscoDataset(Dataset):\n",
    "    def __init__(self, df, skill_col, backbone):\n",
    "        texts = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(backbone)\n",
    "        self.texts = texts[skill_col].values.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        res = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=20\n",
    "        )\n",
    "        return {k:v[0] for k,v in res.items()}\n",
    "\n",
    "    \n",
    "class ClsPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # batch * num_tokens * num_embedding\n",
    "        return x[:, 0, :]    \n",
    "\n",
    "    \n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        self.backbone = AutoModel.from_pretrained(backbone)\n",
    "        self.pool = ClsPool()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(**x)[\"last_hidden_state\"]\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JobBERT\n",
    "JobBERT is a BERT model pre-trained on job propositions, it came from a paper where they concluded that a domain-specific pretrained model outperformed the non-adapted versions and published their model on [Huggingface](https://huggingface.co/jjzha/jobbert-base-cased). I will use it to create embeddings of ESCO skills, then embed the job postings and find relevant ESCO skills using vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "backbone = 'jjzha/jobbert-base-cased'\n",
    "emb_label = 'jobbert'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and Dataloader\n",
    "ds = EscoDataset(esco_df, 'label_cleaned', backbone)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32)\n",
    "\n",
    "# Build custom model\n",
    "model = BertModel(backbone)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Get embeddings for each skill\n",
    "embs = []\n",
    "with torch.no_grad():\n",
    "    for i, x in enumerate(dl):\n",
    "        x = {k:v.to(device) for k, v in x.items()}\n",
    "        out = model(x)\n",
    "        embs.extend(out.detach().cpu())\n",
    "# Add them to the DataFrame\n",
    "esco_df[emb_label] = embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(job):\n",
    "    \"\"\"\n",
    "    Given a raw html job description, parse it into sentences\n",
    "    by using nltk's sentence tokenization + new line splitting, this can also accept raw text not only html text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(job, 'html.parser')\n",
    "    # Found some ads using unicode bullet points\n",
    "    for p in soup.find_all('p'):\n",
    "        p.string = p.get_text().replace(\"•\", \"\")\n",
    "    text = soup.get_text()\n",
    "    st = sent_tokenize(text)\n",
    "    sentences = []\n",
    "    for sent in st:\n",
    "        sentences.extend([x for x in sent.split('\\n') if x !=''])\n",
    "    return sentences\n",
    "\n",
    "def compute_similarity(vec, emb_type):\n",
    "    \"\"\"\n",
    "    Compute vector similarity for a given vec and all the ESCO skills embeddings.\n",
    "    If more embeddings were created, the type is specified by the input parameter.\n",
    "    Return the ESCO skill id with max similarity\n",
    "    \"\"\"\n",
    "    esco_embs = esco_df[emb_type]\n",
    "    sims = []\n",
    "    # Compute cosine similarities\n",
    "    for i, esco_vec in enumerate(esco_embs):\n",
    "        sims.append((i, cosine_similarity(vec, esco_vec.reshape(1, -1))))\n",
    "    # Return max similarity and esco skill index\n",
    "    idx, sim = max(sims, key=lambda x: x[1])\n",
    "    return idx, sim.item()\n",
    "\n",
    "\n",
    "def compute_similarity_opt(emb_vec, emb_type):\n",
    "    \"\"\"\n",
    "    Compute vector similarity for a given vec and all the ESCO skills embeddings\n",
    "    by constructing a matrix from ESCO embeddings to process it faster.\n",
    "    Return the ESCO skill id with max similarity\n",
    "    \"\"\"\n",
    "    esco_embs = [x for x in esco_df[emb_type]]\n",
    "    esco_vectors = torch.stack(esco_embs)\n",
    "    # Normalize the stacked embeddings and the input vector\n",
    "    norm_esco_vectors = torch.nn.functional.normalize(esco_vectors, p=2, dim=1)\n",
    "    norm_emb_vec = torch.nn.functional.normalize(emb_vec.T, p=2, dim=0)\n",
    "    # Compute cosine similarities\n",
    "    cos_similarities = torch.matmul(norm_esco_vectors, norm_emb_vec)\n",
    "    # Return max similarity and esco skill index\n",
    "    sim, idx = torch.max(cos_similarities, dim=0)\n",
    "    return idx.item(), sim.item()\n",
    "\n",
    "def compute_similarity_mat(emb_mat, emb_type):\n",
    "    esco_embs = [x for x in esco_df[emb_type]]\n",
    "    esco_vectors = torch.stack(esco_embs)\n",
    "    emb_vectors = torch.stack(emb_mat)\n",
    "    # Normalize the stacked embeddings and the input vectors\n",
    "    norm_esco_vectors = torch.nn.functional.normalize(esco_vectors, p=2, dim=1)\n",
    "    norm_emb_vecs = torch.nn.functional.normalize(emb_vectors.T, p=2, dim=0)\n",
    "    # Compute cosine similarities\n",
    "    cos_similarities = torch.matmul(norm_esco_vectors, norm_emb_vecs)\n",
    "    # Return max similarity and esco skill index\n",
    "    max_similarities, max_indices = torch.max(cos_similarities, dim=0)\n",
    "    return max_indices.numpy(), max_similarities.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(x):\n",
    "    x = tokenizer(x, return_tensors='pt')\n",
    "    x = {k:v.to(device) for k, v in x.items()}\n",
    "    return model(x).detach().cpu()\n",
    "\n",
    "def process_sentence(sent):\n",
    "    emb = get_embedding(sent)\n",
    "    return compute_similarity_opt(emb, emb_label)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone)\n",
    "model = BertModel(backbone)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Used in performance optimization and output example\n",
    "job_sample = df.iloc[15]['Job Description']\n",
    "threshold = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance optimization\n",
    "By improving the similarity calculation (using tensor operations) on one random job sample, the processing time for all the sentences went down from around 160 seconds to 2 seconds. (A different sample showed improvement from 300 to 5 seconds. The time improvement value is not exact but the improvement is significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-by-one similarity execution time: 177.9822 seconds\n",
      "Optimized similarity execution time: 1.3570 seconds\n"
     ]
    }
   ],
   "source": [
    "sentences = get_sentences(job_sample)\n",
    "\n",
    "# Simple similarity\n",
    "sim_start_time = time.time()\n",
    "for sent in sentences:\n",
    "    x = tokenizer(sent, return_tensors='pt')\n",
    "    x = {k:v.to(device) for k, v in x.items()}\n",
    "    emb = model(x).detach().cpu()\n",
    "    idx, sim = compute_similarity(emb.numpy(), emb_label)\n",
    "\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"One-by-one similarity execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "# Optimized similarity\n",
    "sim_start_time = time.time()\n",
    "for sent in sentences:\n",
    "    x = tokenizer(sent, return_tensors='pt')\n",
    "    x = {k:v.to(device) for k, v in x.items()}\n",
    "    emb = model(x).detach().cpu()\n",
    "    idx, sim = compute_similarity_opt(emb, emb_label)\n",
    "\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"Optimized similarity execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further optimization\n",
    "I edited the method once again to compute the similarity between two matrices (matrix of sentence embeddings and a matrix of esco embeddings) in one operation. This further enhanced the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.7253 seconds\n"
     ]
    }
   ],
   "source": [
    "sentences = get_sentences(job_sample)\n",
    "\n",
    "sim_start_time = time.time()\n",
    "sent_embs = []\n",
    "for sent in sentences:\n",
    "    x = tokenizer(sent, return_tensors='pt')\n",
    "    x = {k:v.to(device) for k, v in x.items()}\n",
    "    emb = model(x).detach().cpu()\n",
    "    sent_embs.append(emb.squeeze())\n",
    "idxs, sims = compute_similarity_mat(sent_embs, emb_label)\n",
    "# Calculate job description processing time\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example with ESCO mapping outputs**\n",
    "\n",
    "I am using a threshold parameter to filter out unlikely matches. We can see that the sentence-wise detection made some mistakes but the mapped skills are not entirely irrelevant. I also display speeds for both \"fast\" similarity calculation approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.6109 seconds\n",
      "=========================\n",
      "sentence: Contribute to & maintain open source projects\n",
      "ESCO skill:operate open source software\n",
      "Similarity:0.8464\n",
      "=========================\n",
      "sentence: Familiar with agile methodologies\n",
      "ESCO skill:Agile development\n",
      "Similarity:0.8328\n",
      "=========================\n",
      "sentence: Experience with design and development of backend services\n",
      "ESCO skill:implement front-end website design\n",
      "Similarity:0.8274\n",
      "=========================\n",
      "sentence: Experience with software testing methodologies\n",
      "ESCO skill:levels of software testing\n",
      "Similarity:0.8499\n",
      "=========================\n",
      "sentence: Contributions to open-source projects\n",
      "ESCO skill:Open source model\n",
      "Similarity:0.8590\n"
     ]
    }
   ],
   "source": [
    "sim_start_time = time.time()\n",
    "res = []\n",
    "sentences = get_sentences(job_sample)\n",
    "for sent in sentences:\n",
    "    idx, sim = process_sentence(sent)\n",
    "    if sim > threshold:\n",
    "        res.append((sent, esco_df.iloc[idx]['label_cleaned'], sim))\n",
    "\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "for r in res:\n",
    "    print('=========================')\n",
    "    print(f\"sentence: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kochava builds real-time tracking and attribution analytics tools for connected devices; serving the world’s top brands and apps.',\n",
       " 'We analyze millions of requests every single day and are ramping up at an extraordinary pace to serve billions of requests every day.',\n",
       " 'The company is growing fast as we add new clients and services and we are looking to add talented, dedicated and innovative people who will strengthen our core team.',\n",
       " \"We're looking for a Senior Software Engineer to function as a central member of Kochava’s XCHNG development team.\",\n",
       " 'The digital advertising blockchain market is challenging, dynamic, fun, and provides almost unlimited opportunities for personal and professional growth.',\n",
       " 'We are looking for dedicated team players who are comfortable with self-direction, and inspired by the thrill of building creative solutions to challenging problems.',\n",
       " 'Several of our programmers are self-taught, a degree is not a necessity.',\n",
       " 'A love of programming and ability to work in a team are.',\n",
       " 'Job Responsibilities',\n",
       " 'Be a core member of the team implementing distributed applications',\n",
       " 'Contribute to & maintain open source projects',\n",
       " 'Design & implement highly-available, fault-tolerant, real time, decentralized systems',\n",
       " 'Balance a fast paced delivery schedule while ensuring quality and resilience',\n",
       " 'Maintenance, care, and optimization of systems',\n",
       " 'Experience/Skills Required',\n",
       " 'B.S/M.S Computer Science degree or equivalent experience',\n",
       " 'Minimum 5+ years of software engineering experience',\n",
       " 'Experience leading a team and a history of fulfilling project requirements',\n",
       " 'Familiar with agile methodologies',\n",
       " 'Experience with design and development of backend services',\n",
       " 'Experience in one or more of the following languages Golang, Java, Python, Javascript (Strong preference for Golang)',\n",
       " 'Familiar with Solidity or other smart contract oriented languages',\n",
       " 'Comfortable with full stack development',\n",
       " 'Bonus',\n",
       " 'Experience with software testing methodologies',\n",
       " 'Contributions to open-source projects',\n",
       " 'Gitlab CI & Jenkins',\n",
       " 'Docker',\n",
       " 'Passionate about innovation',\n",
       " 'Our Start',\n",
       " 'Kochava began in 2011 when a team of mobile and gaming professionals saw the need to better understand the feedback loop of user acquisition, engagement, and LTV for mobile applications.',\n",
       " 'Through the process of creating apps for customers from a wide range of industries, we were repeatedly asked if we could shed some light on what media advertising efforts were converting and the effectiveness of their mobile ad spend by partner.',\n",
       " 'Realizing a solution to these questions wasn’t readily available, we started designing and building a mobile measurement platform that would become Kochava.',\n",
       " 'Our HQ Location',\n",
       " 'We live and work in the wonderful ski resort town of Sandpoint, Idaho.',\n",
       " 'Kochava has sales and agency representation in major markets worldwide, but we choose to grow our development and engineering infrastructure around Sandpoint, as it allows us to stay focused on cutting code while taking advantage of the community and quality of life the town offers to our team.',\n",
       " 'Kochava is an equal opportunity employer committed to building a team culture that celebrates diversity and inclusion.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(job_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.4956 seconds\n",
      "=========================\n",
      "sentence: Contribute to & maintain open source projects\n",
      "ESCO skill:operate open source software\n",
      "Similarity:0.8464\n",
      "=========================\n",
      "sentence: Familiar with agile methodologies\n",
      "ESCO skill:Agile development\n",
      "Similarity:0.8328\n",
      "=========================\n",
      "sentence: Experience with design and development of backend services\n",
      "ESCO skill:implement front-end website design\n",
      "Similarity:0.8274\n",
      "=========================\n",
      "sentence: Experience with software testing methodologies\n",
      "ESCO skill:levels of software testing\n",
      "Similarity:0.8499\n",
      "=========================\n",
      "sentence: Contributions to open-source projects\n",
      "ESCO skill:Open source model\n",
      "Similarity:0.8590\n"
     ]
    }
   ],
   "source": [
    "sim_start_time = time.time()\n",
    "res = []\n",
    "sent_embs = []\n",
    "sentences = get_sentences(job_sample)\n",
    "for sent in sentences:\n",
    "    sent_embs.append(get_embedding(sent).squeeze())\n",
    "    \n",
    "idxs, sims = compute_similarity_mat(sent_embs, emb_label)\n",
    "for i in range(len(idxs)):\n",
    "    if sims[i] > threshold:\n",
    "        res.append((sentences[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n",
    "\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "for r in res:\n",
    "    print('=========================')\n",
    "    print(f\"sentence: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried both JobBERT and EscoXLMR but it seemed to me EscoXMLR had some problems with respresenting found spans correctly, moreover I am already using JobBERT embeddings so I opted for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifiers(mtype):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if mtype == \"jobbert\":\n",
    "        token_skill_classifier = pipeline(model=\"jjzha/jobbert_skill_extraction\", aggregation_strategy=\"first\", device=device)\n",
    "        token_knowledge_classifier = pipeline(model=\"jjzha/jobbert_knowledge_extraction\", aggregation_strategy=\"first\", device=device)\n",
    "    elif mtype == \"xlmr\":        \n",
    "        token_skill_classifier = pipeline(model=\"jjzha/escoxlmr_skill_extraction\", aggregation_strategy=\"first\", device=device)\n",
    "        token_knowledge_classifier = pipeline(model=\"jjzha/escoxlmr_knowledge_extraction\", aggregation_strategy=\"first\", device=device)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model name provided\")\n",
    "    return token_skill_classifier, token_knowledge_classifier\n",
    "\n",
    "\n",
    "def extract_skills(job, token_skill_classifier, token_knowledge_classifier, out_treshold=.8, sim_threshold=.8):\n",
    "    \"\"\"\n",
    "    Function that processes outputs from pre-trained, ready to use models\n",
    "    that detect skills as a token classification task. There are two thresholds,\n",
    "    out_threshold for filtering model outputs and sim_threshold for filtering\n",
    "    based on vector similarity with ESCO skills\n",
    "    \"\"\"     \n",
    "    sentences = get_sentences(job)\n",
    "    pred_labels = []\n",
    "    res = []\n",
    "    skill_embs = []\n",
    "    skill_texts = []\n",
    "    for sent in sentences:\n",
    "        skills = ner(sent, token_skill_classifier, token_knowledge_classifier)\n",
    "        for entity in skills['entities']:\n",
    "            text = entity['word']\n",
    "            if entity['score'] > out_treshold:\n",
    "                skill_embs.append(get_embedding(text).squeeze())\n",
    "                skill_texts.append(text)\n",
    "                \n",
    "    idxs, sims = compute_similarity_mat(skill_embs, emb_label)\n",
    "    for i in range(len(idxs)):\n",
    "        if sims[i] > sim_threshold:\n",
    "            pred_labels.append(idxs[i])\n",
    "            res.append((skill_texts[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n",
    "    return pred_labels, res\n",
    "\n",
    "\n",
    "def aggregate_span(results):\n",
    "    new_results = []\n",
    "    current_result = results[0]\n",
    "\n",
    "    for result in results[1:]:\n",
    "        if result[\"start\"] == current_result[\"end\"] + 1:\n",
    "            current_result[\"word\"] += \" \" + result[\"word\"]\n",
    "            current_result[\"end\"] = result[\"end\"]\n",
    "        else:\n",
    "            new_results.append(current_result)\n",
    "            current_result = result\n",
    "\n",
    "    new_results.append(current_result)\n",
    "\n",
    "    return new_results\n",
    "\n",
    "\n",
    "def ner(text, token_skill_classifier, token_knowledge_classifier):\n",
    "    output_skills = token_skill_classifier(text)\n",
    "    for result in output_skills:\n",
    "        if result.get(\"entity_group\"):\n",
    "            result[\"entity\"] = \"Skill\"\n",
    "            del result[\"entity_group\"]\n",
    "\n",
    "    output_knowledge = token_knowledge_classifier(text)\n",
    "    for result in output_knowledge:\n",
    "        if result.get(\"entity_group\"):\n",
    "            result[\"entity\"] = \"Knowledge\"\n",
    "            del result[\"entity_group\"]\n",
    "\n",
    "    if len(output_skills) > 0:\n",
    "        output_skills = aggregate_span(output_skills)\n",
    "    if len(output_knowledge) > 0:\n",
    "        output_knowledge = aggregate_span(output_knowledge)\n",
    "    \n",
    "    skills = []\n",
    "    skills.extend(output_skills)\n",
    "    skills.extend(output_knowledge)\n",
    "    return {\"text\": text, \"entities\": skills}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this approach catches much more skills as it works on token-level. Most of them are also correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9b6555f45d41d896e424be68caed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdd11987fea425199057926e86bd975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935bba664496464c8b177d649d041155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f8db4dcd7b4c0dbf6971a137639978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff56620f73d4451bca4b6cef93212b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a183c3af564d4d988f205cbe904366f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2577f3c09db432191dccd2fe7a7f588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70e517deb60431a95235688aa53ee05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11fec96a3ae4d5eabb50e5eed80aad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734ffa82fb7c4d89881f5bdc17c01795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c894bddd8e049239621ba5e5e9872a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/jjzha/jobbert_knowledge_extraction/resolve/main/tokenizer.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73faa539efcc4cba96e36e0f4afa7b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9485cfb97cf4459892e6251632a2d0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\logging\\__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\logging\\__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\logging\\__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\logging\\__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\rayen\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\rayen\\AppData\\Local\\Temp\\ipykernel_14040\\3019258179.py\", line 4, in <module>\n",
      "    _, res = extract_skills(job_sample, tsc, tkc)\n",
      "  File \"C:\\Users\\rayen\\AppData\\Local\\Temp\\ipykernel_14040\\3504395.py\", line 27, in extract_skills\n",
      "    skills = ner(sent, token_skill_classifier, token_knowledge_classifier)\n",
      "  File \"C:\\Users\\rayen\\AppData\\Local\\Temp\\ipykernel_14040\\3504395.py\", line 60, in ner\n",
      "    output_skills = token_skill_classifier(text)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py\", line 248, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"C:\\Users\\rayen\\spark_env\\Lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.4607 seconds\n",
      "=========================\n",
      "text: self - direction\n",
      "ESCO skill:self-promote\n",
      "Similarity:0.8803\n",
      "=========================\n",
      "text: building creative solutions to challenging problems\n",
      "ESCO skill:create solutions to problems\n",
      "Similarity:0.8554\n",
      "=========================\n",
      "text: work in a team\n",
      "ESCO skill:work in a landscape team\n",
      "Similarity:0.9260\n",
      "=========================\n",
      "text: Contribute to & maintain open source projects\n",
      "ESCO skill:operate open source software\n",
      "Similarity:0.8464\n",
      "=========================\n",
      "text: open source projects\n",
      "ESCO skill:Open source model\n",
      "Similarity:0.9168\n",
      "=========================\n",
      "text: software engineering\n",
      "ESCO skill:computer engineering\n",
      "Similarity:0.8377\n",
      "=========================\n",
      "text: leading a team\n",
      "ESCO skill:lead a team\n",
      "Similarity:0.9468\n",
      "=========================\n",
      "text: agile methodologies\n",
      "ESCO skill:Agile development\n",
      "Similarity:0.8723\n",
      "=========================\n",
      "text: design and development of backend services\n",
      "ESCO skill:implement front-end website design\n",
      "Similarity:0.8291\n",
      "=========================\n",
      "text: backend\n",
      "ESCO skill:JavaScript\n",
      "Similarity:0.8135\n",
      "=========================\n",
      "text: Java\n",
      "ESCO skill:Java\n",
      "Similarity:1.0000\n",
      "=========================\n",
      "text: Python\n",
      "ESCO skill:Python\n",
      "Similarity:1.0000\n",
      "=========================\n",
      "text: Javascript\n",
      "ESCO skill:JavaScript\n",
      "Similarity:0.9068\n",
      "=========================\n",
      "text: Solidity\n",
      "ESCO skill:Solidity\n",
      "Similarity:1.0000\n",
      "=========================\n",
      "text: software testing methodologies\n",
      "ESCO skill:software design methodologies\n",
      "Similarity:0.9010\n",
      "=========================\n",
      "text: open - source\n",
      "ESCO skill:Open source model\n",
      "Similarity:0.9089\n",
      "=========================\n",
      "text: Jenkins\n",
      "ESCO skill:Jenkins\n",
      "Similarity:1.0000\n"
     ]
    }
   ],
   "source": [
    "tsc, tkc = get_classifiers(\"jobbert\")\n",
    "\n",
    "start_time = time.time()\n",
    "_, res = extract_skills(job_sample, tsc, tkc)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "for r in res:\n",
    "    print('=========================')\n",
    "    print(f\"text: {r[0]}\\nESCO skill:{r[1]}\\nSimilarity:{r[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small labelled data subset\n",
    "My task was to label a subset of data, I found out that the task was harder than I expected so I ended up manually labelling only 4 randomly selected job ads. For the labelling approach, I chose to use index names of ESCO skills as labels. It is true that this might mean that some skills, that are present in the description will not be picked up, but the outputs will be more uniform.\n",
    "\n",
    "Another approach I though about was to label the data on span-level with the help of a labeling tool to make it faster and easier. I would then use just the parts of text detected as skills instead of ESCO terms. This might have helped to better extract concrete skills rather than generalized ontology terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in preds.items():\n",
    "        target = labels[k] \n",
    "        # Calculate TP, FP, FN for the current entry\n",
    "        tp += sum(1 for i in range(len(v)) if v[i] in target)\n",
    "        fp += sum(1 for i in range(len(v)) if v[i] not in target)\n",
    "        fn += sum(1 for i in range(len(target)) if target[i] not in v)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for the current entry\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyber IT Risk & Strategy Senior Consultant \n",
      " ['cyber security', 'cyber attack counter-measures', 'implement ICT risk management', 'risk management', 'perform business analysis', 'apply risk management processes', 'perform church service', 'DevOps', \"assess risks of clients' assets\", 'present reports', 'develop information security strategy', 'implement strategic planning', 'identify technological needs', 'consult with business clients', 'project management', 'advise client on technical possibilities'] \n",
      "-----------------------------\n",
      "AV Systems Drawings AutoCAD Engineer \n",
      " ['design electrical systems', 'create AutoCAD drawings', 'use CAD software', 'technical drawings', 'mathematics'] \n",
      "-----------------------------\n",
      "Pre-owned Auto Sales Consultant \n",
      " ['advise customers on motor vehicles', \"identify customer's needs\", 'satisfy customers', 'communication', 'Spanish', 'understand spoken Spanish'] \n",
      "-----------------------------\n",
      "Senior Java Developer \n",
      " ['Java', 'Python', 'Groovy', 'JavaScript', 'develop software prototype', 'Apache Maven', 'perform software unit testing', 'systems development life-cycle', 'Ruby', 'NoSQL'] \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.loc[[5, 211, 434, 6141]]\n",
    "df_sample['labels'] = [\n",
    "    [7049, 4850, 5814, 6104, 2180, 8242, 4893, 13032, 3453, 11317, 2966, 13431, 3654, 8186, 6224, 6762],\n",
    "    [233, 6498, 9743, 4922, 3673],\n",
    "    [13557, 5734, 7203, 1166, 1121, 8793],\n",
    "    [1370, 11127, 4544, 3338, 6670, 699, 6667, 521, 680, 6535]\n",
    "]\n",
    "df_sample[['Job Description', 'labels']].to_csv(\"df_sample_labeled.csv\")\n",
    "\n",
    "for index, s in df_sample.iterrows():\n",
    "    esco_skills = [esco_df.loc[x]['label_cleaned'] for x in s['labels']]\n",
    "    print(s['Job Title'], '\\n', esco_skills, \"\\n-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.6414 seconds\n",
      "Precision: 0.1538, Recall: 0.0541, F1-score: 0.0800\n"
     ]
    }
   ],
   "source": [
    "threshold = .8\n",
    "preds = {}\n",
    "res_log = {}\n",
    "sim_start_time = time.time()\n",
    "for index, s in df_sample.iterrows():\n",
    "    res = []\n",
    "    pred_labels = []\n",
    "    sent_embs = []\n",
    "    sentences = get_sentences(s['Job Description'])\n",
    "    for sent in sentences:\n",
    "        sent_embs.append(get_embedding(sent).squeeze())\n",
    "        \n",
    "    idxs, sims = compute_similarity_mat(sent_embs, emb_label)\n",
    "    for i in range(len(idxs)):\n",
    "        if sims[i] > threshold:\n",
    "            pred_labels.append(idxs[i])\n",
    "            res.append((sentences[i], esco_df.iloc[idxs[i]]['label_cleaned'], sims[i]))\n",
    "    # Save results\n",
    "    preds[index] = list(set(pred_labels))\n",
    "    res_log[index] = list(set(res))\n",
    "# Calculate job description processing time\n",
    "sim_end_time = time.time()\n",
    "execution_time = sim_end_time - sim_start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "calculate_metrics(preds, df_sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4.7214 seconds\n",
      "Precision: 0.3167, Recall: 0.5135, F1-score: 0.3918\n"
     ]
    }
   ],
   "source": [
    "out_treshold = .8\n",
    "sim_treshold = .8\n",
    "preds = {}\n",
    "res_log = {}\n",
    "start_time = time.time()\n",
    "for index, s in df_sample.iterrows():\n",
    "    pred_labels, res = extract_skills(s['Job Description'], tsc, tkc)\n",
    "    # Save results\n",
    "    preds[index] = list(set(pred_labels))\n",
    "    res_log[index] = list(set(res))\n",
    "# Calculate job description processing time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "calculate_metrics(preds, df_sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Nesta skill extractor library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nesta skill extractor library uses the same architecture and the basically functions the same way, the only problem is that it captures vague and general topic unlike our customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:37:11.293394Z",
     "iopub.status.busy": "2024-04-16T16:37:11.292886Z",
     "iopub.status.idle": "2024-04-16T16:37:54.131121Z",
     "shell.execute_reply": "2024-04-16T16:37:54.129619Z",
     "shell.execute_reply.started": "2024-04-16T16:37:11.293359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nestauk/ojd_daps_skills.git@dev\n",
      "  Cloning https://github.com/nestauk/ojd_daps_skills.git (to revision dev) to /tmp/pip-req-build-qop_76cg\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nestauk/ojd_daps_skills.git /tmp/pip-req-build-qop_76cg\n",
      "  Resolved https://github.com/nestauk/ojd_daps_skills.git to commit 6b94921f3173ed6686d574aad76567013cd24b89\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.24.4 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.24.4)\n",
      "Requirement already satisfied: scipy==1.10.1 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.10.1)\n",
      "Requirement already satisfied: pandas==1.3.5 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.3.5)\n",
      "Requirement already satisfied: tqdm==4.64.0 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (4.64.0)\n",
      "Requirement already satisfied: filelock==3.7.1 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (3.7.1)\n",
      "Requirement already satisfied: typer==0.4.1 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (0.4.1)\n",
      "Requirement already satisfied: sh==1.14.2 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.14.2)\n",
      "Requirement already satisfied: transformers==4.33.3 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (4.33.3)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn==1.3.1 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.3.1)\n",
      "Requirement already satisfied: spacy==3.4.0 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (3.4.0)\n",
      "Requirement already satisfied: nervaluate==0.1.8 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (0.1.8)\n",
      "Requirement already satisfied: s3fs==2022.5.0 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (2022.5.0)\n",
      "Requirement already satisfied: boto3==1.21.21 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (1.21.21)\n",
      "Requirement already satisfied: toolz==0.12.0 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0 in /opt/conda/lib/python3.10/site-packages (from ojd_daps_skills==1.0.3.dev2+g6b94921) (4.5.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.21 in /opt/conda/lib/python3.10/site-packages (from boto3==1.21.21->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.24.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3==1.21.21->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from boto3==1.21.21->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.3.5->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.3.5->ojd_daps_skills==1.0.3.dev2+g6b94921) (2023.3)\n",
      "Requirement already satisfied: aiobotocore~=2.3.0 in /opt/conda/lib/python3.10/site-packages (from s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.3.4)\n",
      "Requirement already satisfied: fsspec==2022.5.0 in /opt/conda/lib/python3.10/site-packages (from s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2022.5.0)\n",
      "Requirement already satisfied: aiohttp<=4 in /opt/conda/lib/python3.10/site-packages (from s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.8.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.1->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.3.1->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.15.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.2.4)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.17.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.3->ojd_daps_skills==1.0.3.dev2+g6b94921) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.3->ojd_daps_skills==1.0.3.dev2+g6b94921) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.3->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.3->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.4.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer==0.4.1->ojd_daps_skills==1.0.3.dev2+g6b94921) (8.1.7)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.3.0->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.15.0)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.3.0->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.11.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=4->s3fs==2022.5.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.25.0,>=1.24.21->boto3==1.21.21->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.26.15)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (6.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (0.1.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy==3.4.0->ojd_daps_skills==1.0.3.dev2+g6b94921) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (10.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2->ojd_daps_skills==1.0.3.dev2+g6b94921) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/nestauk/ojd_daps_skills.git@dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:37:54.133671Z",
     "iopub.status.busy": "2024-04-16T16:37:54.133263Z",
     "iopub.status.idle": "2024-04-16T16:38:17.376652Z",
     "shell.execute_reply": "2024-04-16T16:38:17.375387Z",
     "shell.execute_reply.started": "2024-04-16T16:37:54.133633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl#egg=en_core_web_sm==3.4.1 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:38:37.438553Z",
     "iopub.status.busy": "2024-04-16T16:38:37.438063Z",
     "iopub.status.idle": "2024-04-16T16:39:06.794307Z",
     "shell.execute_reply": "2024-04-16T16:39:06.792349Z",
     "shell.execute_reply.started": "2024-04-16T16:38:37.438484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20;1;1m2024-04-16 16:38:42,825 - SkillsExtractor - WARNING - Neccessary files are not downloaded. Downloading ~1GB of neccessary files. (extract_skills.py:63)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:39:00,583 - SkillsExtractor - INFO - Data folder downloaded from /opt/conda/lib/python3.10/site-packages/ojd_daps_skills_data (download_public_data.py:29)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:39:00,587 - SkillsExtractor - INFO - Loading the model from a local location (ner_spacy.py:507)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:39:00,588 - SkillsExtractor - INFO - Loading the model from /opt/conda/lib/python3.10/site-packages/ojd_daps_skills_data/outputs/models/ner_model/20230808/ (ner_spacy.py:510)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:39:03,780 - SkillsExtractor - INFO - Loading 'toy' taxonomy information (extract_skills.py:154)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c83bc93c5044ee900dce26968e05c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a3f9ee8e934b88b54afbe7861eabba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677f926335a74a9ba5fa6bcaf2175163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476115fa4eb74302b2d842b0fdd0cbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c194af3ab65489d82ca77945ff8d9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3111c30d804f12afcec4ee3d57c3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc184f6205f4618a06b2aa9ca590f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593b76d4030549a7b3cf8b27bdf40e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88153af5c744ff98ff73c93026dffff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94dce063dd8474e878c95fdab1d027f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7dd0db8ec24dccaf75cbdd3b0b90bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d83ba59978a458cb1bb26ed0a699a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff51a91418e46ed9c45bfe24d15e56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebda41b12324dcdb46eb8f5ebb88aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4526f13eaa548c29e53a9df7ec9fac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ojd_daps_skills.pipeline.extract_skills.extract_skills import ExtractSkills #import the module\n",
    "\n",
    "es = ExtractSkills(config_name=\"extract_skills_toy\", local=True) #instantiate with toy taxonomy configuration file\n",
    "\n",
    "es.load() #load necessary models\n",
    "\n",
    "job_adverts = [\n",
    "    \"The job involves communication skills and maths skills\",\n",
    "    \"The job involves Excel skills. You will also need good presentation skills\"\n",
    "] #toy job advert examples\n",
    "\n",
    "predicted_skills = es.get_skills(job_adverts) #extract skills from list of job adverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:57:16.694679Z",
     "iopub.status.busy": "2024-04-16T16:57:16.693903Z",
     "iopub.status.idle": "2024-04-16T16:57:16.704383Z",
     "shell.execute_reply": "2024-04-16T16:57:16.703213Z",
     "shell.execute_reply.started": "2024-04-16T16:57:16.694641Z"
    }
   },
   "outputs": [],
   "source": [
    "job = \"\"\"Education : related courses : \n",
    "- statistical analysis \n",
    "- machine learning \n",
    "- deep learning \n",
    "- cloud services  \n",
    "- probability and statistics \n",
    "Experience : \n",
    "machine learning intern : \n",
    "● Identified business challenges and opportunities within the recruitment process and applied Natural Language\n",
    "Processing techniques to develop an innovative Application Tracking System. This system efficiently ranked\n",
    "candidates for specific job descriptions, using named entity recognition and word embedding, resulting in a\n",
    "significant reduction in the time spent on candidate selection.\n",
    "● Orchestrated a comprehensive data acquisition strategy to harvest and preprocess candidate application data\n",
    "with precision.\n",
    "● Collaborated closely with HR and engineering teams to deploy the system using FastAPI and Vue.js, ensuring a\n",
    "user-friendly interface for recruiters, and precisely mapped out a plan for the next 2 upcoming versions and\n",
    "enhancements.\n",
    "Buisness developer at enactus fst el manar : \n",
    "●Contributed to product creation, market evaluation, and customer segmentation while driving\n",
    "clients' behavior analysis.\n",
    "● Collaborated effectively with the sales team to develop a robust sales strategy for optimal results in 2 different\n",
    "projects : Moonray, and Student plus\n",
    "Junior machine learning engineer at omdena : \n",
    "● Played a pivotal role in the successful development and deployment of a computer vision system using deep\n",
    "learning techniques ( pytorch ) in Egyptian orphanages, effectively tackling the issue of constrained monitoring.\n",
    "● Gained expertise in deploying accurate solutions with low computational capabilities using python.\n",
    "● Helped improve the well-being of more than 220 children.\n",
    "Data analyst : omdena : \n",
    "● Successfully extracted and delivered actionable insights from a diverse dataset as part of the Omdena initiative for Peru's Open Data Platform, enabling positive transformation in Lima and significantly enhancing the quality of life for its residents. \n",
    "● Applied Python techniques for data collection and preprocessing, and Power BI for visualization dashboards, yielding critical insights into aggression trends and contributing to targeted interventions in Lima.\n",
    "IT consultant at optima junior entreprise:  \n",
    "● Utilized Vue.js, Laravel, and MySQL to successfully aggregate diverse data sources, standardize data storage, and develop interactive dashboards, leading to enhanced data management efficiency, elevated data quality, and user-friendly data accessibility.\n",
    "Data science unity manager engineers spark : \n",
    "● Contributed To building a training strategy In the field of Data science for college students. \n",
    "● Executed 7 training sessions which significantly elevated students' proficiency in data science, achieving a remarkable 60% knowledge enhancement, coupled with an impressive satisfaction rate exceeding 85%.\n",
    "Projects : \n",
    "Student performance prediction and analysis : \n",
    "Utilized and combined various machine learning techniques such as bagging and boosting with Python to predict\n",
    "the performance of high school students.\n",
    "● Employed Local and global interpretation techniques with Python to conduct in-depth analysis of the\n",
    "inter-relations among various educational factors for performance prediction.\n",
    "● Conducted A/B testing on a cohort of 1000 high school students to assess the impact of different educational\n",
    "interventions and refine performance prediction models. \n",
    "Sales analysis : \n",
    "This project analyzes sales data with 3 interactive and clear dashboards. It summarizes sales indicators, highlights customer preferences, and details product categories/specifications driving valuable insights to inform business decisions. \n",
    "Virtual accounting firm : \n",
    "The platform aims to automate a workflow using angular. Additionally, it allows for better tracking of the accounting firm's resources and ensures more accurate and transparent billing for clients. It operates as a virtual office. Therefore, the principle is to automate various tasks that are typically performed in a \"physical\" office.\"\"\",\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:53:53.630307Z",
     "iopub.status.busy": "2024-04-16T16:53:53.629309Z",
     "iopub.status.idle": "2024-04-16T16:53:54.365099Z",
     "shell.execute_reply": "2024-04-16T16:53:54.363558Z",
     "shell.execute_reply.started": "2024-04-16T16:53:53.630266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94;1;1m2024-04-16 16:53:54,162 - SkillsExtractor - INFO - Cleaned job skills (skill_ner_mapper.py:201)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,163 - SkillsExtractor - INFO - Mapping 28 skills to the 'toy' taxonomy (extract_skills.py:332)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,165 - SkillsExtractor - INFO - Getting embeddings for 5 texts ... (bert_vectorizer.py:37)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,288 - SkillsExtractor - INFO - Took 0.12240743637084961 seconds (bert_vectorizer.py:48)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,289 - SkillsExtractor - INFO - Getting embeddings for 28 texts ... (bert_vectorizer.py:37)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,303 - SkillsExtractor - INFO - Took 0.013159513473510742 seconds (bert_vectorizer.py:48)\u001b[0m\n",
      "\u001b[94;1;1m2024-04-16 16:53:54,359 - SkillsExtractor - INFO - Mapped extracted skills onto 'toy' taxonomy (skill_ner_mapper.py:505)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted_skills = es.extract_skills(job) #extract skills from list of job adverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:54:04.547363Z",
     "iopub.status.busy": "2024-04-16T16:54:04.546374Z",
     "iopub.status.idle": "2024-04-16T16:54:04.559128Z",
     "shell.execute_reply": "2024-04-16T16:54:04.557972Z",
     "shell.execute_reply.started": "2024-04-16T16:54:04.547322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {},\n",
       " {'SKILL': [('machine learning', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {'SKILL': [('cloud services', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('machine learning intern', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {'SKILL': [('Processing techniques',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('product creation', ('working with computers', 'S5')),\n",
       "   ('market evaluation',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('Student',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('python', ('working with computers', 'S5')),\n",
       "   ('deploying accurate solutions with low computational capabilities',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('data collection', ('working with computers', 'S5')),\n",
       "   ('Python techniques', ('working with computers', 'S5')),\n",
       "   ('preprocessing', ('working with computers', 'S5')),\n",
       "   ('Power BI for visualization dashboards',\n",
       "    ('working with computers', 'S5'))]},\n",
       " {'SKILL': [('IT consultant',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {'SKILL': [('leading to enhanced data management efficiency, elevated data quality',\n",
       "    ('working with computers', 'S5')),\n",
       "   ('user-friendly data accessibility', ('working with computers', 'S5')),\n",
       "   ('develop interactive dashboards', ('working with computers', 'S5')),\n",
       "   ('MySQL', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('data science', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {'SKILL': [('analysis', ('working with computers', 'S5'))]},\n",
       " {'SKILL': [('machine learning techniques', ('working with computers', 'S5')),\n",
       "   ('Python', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {'SKILL': [('Python', ('working with computers', 'S5')),\n",
       "   ('conduct in-depth analysis',\n",
       "    ('communication, collaboration and creativity', 'S1'))]},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('refine performance prediction models',\n",
       "    ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('inform business decisions', ('working with computers', 'S5'))]},\n",
       " {'SKILL': [('Virtual accounting firm   ', ('working with computers', 'S5'))]},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'SKILL': [('automate various tasks that are typically performed in a \"physical\" office',\n",
       "    ('working with computers', 'S5'))]}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:58:29.297124Z",
     "iopub.status.busy": "2024-04-16T16:58:29.296247Z",
     "iopub.status.idle": "2024-04-16T16:58:31.640594Z",
     "shell.execute_reply": "2024-04-16T16:58:31.639589Z",
     "shell.execute_reply.started": "2024-04-16T16:58:29.297090Z"
    }
   },
   "outputs": [],
   "source": [
    "_, res = extract_skills(\"\"\"Education : related courses : \n",
    "- statistical analysis \n",
    "- machine learning \n",
    "- deep learning \n",
    "- cloud services  \n",
    "- probability and statistics \n",
    "Experience : \n",
    "machine learning intern : \n",
    "● Identified business challenges and opportunities within the recruitment process and applied Natural Language\n",
    "Processing techniques to develop an innovative Application Tracking System. This system efficiently ranked\n",
    "candidates for specific job descriptions, using named entity recognition and word embedding, resulting in a\n",
    "significant reduction in the time spent on candidate selection.\n",
    "● Orchestrated a comprehensive data acquisition strategy to harvest and preprocess candidate application data\n",
    "with precision.\n",
    "● Collaborated closely with HR and engineering teams to deploy the system using FastAPI and Vue.js, ensuring a\n",
    "user-friendly interface for recruiters, and precisely mapped out a plan for the next 2 upcoming versions and\n",
    "enhancements.\n",
    "Buisness developer at enactus fst el manar : \n",
    "●Contributed to product creation, market evaluation, and customer segmentation while driving\n",
    "clients' behavior analysis.\n",
    "● Collaborated effectively with the sales team to develop a robust sales strategy for optimal results in 2 different\n",
    "projects : Moonray, and Student plus\n",
    "Junior machine learning engineer at omdena : \n",
    "● Played a pivotal role in the successful development and deployment of a computer vision system using deep\n",
    "learning techniques ( pytorch ) in Egyptian orphanages, effectively tackling the issue of constrained monitoring.\n",
    "● Gained expertise in deploying accurate solutions with low computational capabilities using python.\n",
    "● Helped improve the well-being of more than 220 children.\n",
    "Data analyst : omdena : \n",
    "● Successfully extracted and delivered actionable insights from a diverse dataset as part of the Omdena initiative for Peru's Open Data Platform, enabling positive transformation in Lima and significantly enhancing the quality of life for its residents. \n",
    "● Applied Python techniques for data collection and preprocessing, and Power BI for visualization dashboards, yielding critical insights into aggression trends and contributing to targeted interventions in Lima.\n",
    "IT consultant at optima junior entreprise:  \n",
    "● Utilized Vue.js, Laravel, and MySQL to successfully aggregate diverse data sources, standardize data storage, and develop interactive dashboards, leading to enhanced data management efficiency, elevated data quality, and user-friendly data accessibility.\n",
    "Data science unity manager engineers spark : \n",
    "● Contributed To building a training strategy In the field of Data science for college students. \n",
    "● Executed 7 training sessions which significantly elevated students' proficiency in data science, achieving a remarkable 60% knowledge enhancement, coupled with an impressive satisfaction rate exceeding 85%.\n",
    "Projects : \n",
    "Student performance prediction and analysis : \n",
    "Utilized and combined various machine learning techniques such as bagging and boosting with Python to predict\n",
    "the performance of high school students.\n",
    "● Employed Local and global interpretation techniques with Python to conduct in-depth analysis of the\n",
    "inter-relations among various educational factors for performance prediction.\n",
    "● Conducted A/B testing on a cohort of 1000 high school students to assess the impact of different educational\n",
    "interventions and refine performance prediction models. \n",
    "Sales analysis : \n",
    "This project analyzes sales data with 3 interactive and clear dashboards. It summarizes sales indicators, highlights customer preferences, and details product categories/specifications driving valuable insights to inform business decisions. \n",
    "Virtual accounting firm : \n",
    "The platform aims to automate a workflow using angular. Additionally, it allows for better tracking of the accounting firm's resources and ensures more accurate and transparent billing for clients. It operates as a virtual office. Therefore, the principle is to automate various tasks that are typically performed in a \"physical\" office.\n",
    "  \"\"\", tsc, tkc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T16:58:37.102147Z",
     "iopub.status.busy": "2024-04-16T16:58:37.101728Z",
     "iopub.status.idle": "2024-04-16T16:58:37.112197Z",
     "shell.execute_reply": "2024-04-16T16:58:37.110984Z",
     "shell.execute_reply.started": "2024-04-16T16:58:37.102114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('statistical analysis', 'apply statistical analysis techniques', 0.8258099),\n",
       " ('machine learning', 'machine learning', 1.0000004),\n",
       " ('cloud services', 'develop with cloud services', 0.86818767),\n",
       " ('probability', 'boxing', 0.99760455),\n",
       " ('statistics', 'statistics', 0.99999964),\n",
       " ('Identified business challenges and opportunities',\n",
       "  'identify new business opportunities',\n",
       "  0.8265768),\n",
       " ('Processing techniques', 'photographic processing techniques', 0.8895103),\n",
       " ('develop', 'spelling', 0.96657646),\n",
       " ('deploy the system', 'install operating system', 0.80585754),\n",
       " ('ensuring a', 'ensure sanitation', 0.80705374),\n",
       " ('driving', 'Slovenian', 0.903703),\n",
       " ('develop a robust sales strategy for optimal results',\n",
       "  'develop revenue generation strategies',\n",
       "  0.8317915),\n",
       " ('development and deployment of a computer vision system',\n",
       "  'design a media integration system',\n",
       "  0.817845),\n",
       " ('computer vision system', 'develop computer vision system', 0.9435278),\n",
       " ('Python', 'Python', 1.0000001),\n",
       " ('data collection', 'data warehouse', 0.8479222),\n",
       " ('standardize data storage', 'normalise data', 0.8379142),\n",
       " ('develop interactive dashboards', 'create data models', 0.8350029),\n",
       " ('MySQL', 'MySQL', 0.99999964),\n",
       " ('building a training strategy', 'develop a translation strategy', 0.885512),\n",
       " ('Data science', 'data mining', 0.8431864),\n",
       " ('data science', 'data mining', 0.85699856),\n",
       " ('machine learning techniques', 'machine learning', 0.91084814),\n",
       " ('machine learning', 'machine learning', 1.0000004),\n",
       " ('Python', 'Python', 1.0000001),\n",
       " ('conduct in - depth analysis of the',\n",
       "  'conduct analysis of ship data',\n",
       "  0.86046016),\n",
       " ('interpretation', 'nutrition', 0.9570179),\n",
       " ('Python', 'Python', 1.0000001),\n",
       " ('interventions', 'medicines', 0.9960444),\n",
       " ('refine performance prediction models',\n",
       "  'build predictive models',\n",
       "  0.8232337)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 820137,
     "sourceId": 1403062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3749643,
     "sourceId": 6488828,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4765006,
     "sourceId": 8074608,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
